---
title: "Maine Lakes Data Group Secchi Depth Trends"
author: "Shana Ederer"
date: "2025-01-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## INTRODUCTION

This code highlights one possible way to conduct a Mann-Kendall trend analysis using Kendall's Tau to quantify the degree of association between Secchi depth values in meters and years (coded as ordered factors). The package involved is "Kendall."

The data were provided by Jeremy Deeds (lake identity not specified). The observations are stored locally in a .csv file on my computer:

"C:\Users\sederer\Documents\Maine Lakes Data Group\2025_01_01_secchi_depth_trends\sdt_fulldataset.csv"

The objectives of the analysis are:

(1) Run a Mann-Kendall trend analysis on yearly averages (May – October) on the full dataset;

(2) Run a Mann-Kendall trend analysis on yearly averages (May – October) on the most recent 10 years in the dataset; and

(3) Run a Mann-Kendall trend analysis on late summer averages (we’ll use August 1 – September 7) on the full dataset.

To compute the yearly average, we will use the mean of means from each relevant month.

References on Mann-Kendall trend analysis:

Implementation in R is developed in Chs. 8, 10, and 12 of Statistical Methods in Water Resources, Helsel et al. 2020:

#### https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf

Helsel, D.R., Hirsch, R.M., Ryberg, K.R., Archfield, S.A., and Gilroy, E.J., 2020, Statistical methods in water resources: U.S. Geological Survey Techniques and Methods, book 4, chap. A3, 458 p., https://doi.org/10.3133/tm4a3. [Supersedes USGS Techniques of Water-Resources Investigations, book 4, chap. A3, version 1.1.]

#### https://vsp.pnnl.gov/help/vsample/design_trend_mann_kendall.htm

This URL provides statistical formulae and publications relevant to the Mann-Kendall test, although the implementation is in software called Visual Sample Plan.


################################################
#### INSTALL AND LOAD PACKAGES              ####                                        
################################################

```{r}
if(!require(tidyverse)) install.packages('tidyverse', repos = 'http://cran.us.r-project.org')
if(!require(egg)) install.packages('egg', repos = 'http://cran.us.r-project.org')
if(!require(knitr)) install.packages('egg', repos = 'http://cran.us.r-project.org')
if(!require(Kendall)) install.packages('egg', repos = 'http://cran.us.r-project.org')
```


```{r}
library(tidyverse)
library(egg)
library(knitr)
library(Kendall)
```


################################################
#### INPUT DATA                             ####                                        
################################################


```{r}
mld <-read_csv("C:/Users/sederer/Documents/Maine Lakes Data Group/2025_01_01_secchi_depth_trends/sdt_fulldataset.csv")
View(mld)
dim(mld)
```
There are 470 observations on Secchi depth for this lake. The first observation was collected in September of 1976 (9/9/1976) and the last observation in August of 2022 (8/29/2022).

For convenience I will rename the variables.

```{r}
# change column names
colnames(mld) <-c("date", "secchi_m")
head(mld)
```

################################################
#### DATA ASSESSMENT AND WRANGLING:         ####
#### TYPE OF VARIABLES (DEFAULT)            ####
################################################

```{r}
# variable type by columns
str(mld$date)
str(mld$secchi_m)
```
```{r}
# check for duplicate rows
duplicated(mld)
```

################################################
#### DATA ASSESSMENT AND WRANGLING:         ####
#### FORMAT TIME DATA                       ####
################################################

```{r}
# convert character data to date data
mld$date <-as.POSIXct(strptime(mld$date, "%m / %d / %Y"))
str(mld$date)
```
```{r}
# check for duplicate sampling dates
duplicated(mld$date)
```
```{r}
# check: missing dates?
summary(mld$date)

```
No missing observations for date.

For convenience, I will create vectors for year and Julian date.

```{r}
# create vector for year
mld$year <-format(as.Date(mld$date), "%Y")
str(mld$year)
```

```{r}
# create vector for Julian date
mld$julian_day <-format(as.Date(mld$date), "%j")
str(mld$julian_day)
```


################################################
#### DATA ASSESSMENT AND WRANGLING          ####
#### SECCHI DEPTHS                          ####
################################################


```{r}
hist(mld$secchi_m, main = "Distribution of Secchi Depths", xlab = "Secchi Depth (m)")
```

```{r}
# check: missing depths?
summary(mld$secchi_m)

```
No missing data for depth.

Before we start visualizing and analyzing the data, we need to round the Secchi depths. The level of reported precision ranges from a tenth of a meter (0.1) to a thousandth of a meter (0.001) in a few cases. Realistically speaking, it is probably not possible to measure Secchi depth so precisely.

So we will round all of the Secchi depths to the nearest 0.1 m.

```{r}
mld$secchi_m <-round(mld$secchi_m, digits = 1)
mld$secchi_m
```

################################################
#### VISUALIZATION                          ####
#### SECCHI DEPTHS                          ####
################################################

Let's look at the Secchi depths by year and Julian date within year.

```{r mld_secchi_by_julian_day_and_year_scatterplot_facet_horiz, fig.height = 25, fig.width = 7, dev.args = list(pointsize = 12)}

# facet_grid plots: secchi ~ julian_day | year
gg <-mld %>%
    group_by(year) %>%
    ggplot(aes(x = julian_day, y = secchi_m)) + geom_point(color = "blue") + facet_grid(rows = vars(year)) + ggtitle("Secchi Depth ~ Julian Day by Year") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 6))
gg

```


Water transparency is typically between 4 and 7 meters; there are a few brief excursions into low transparency conditions (2-3 m) in 1989, 1996, 1998, 2001, 2009, and 2012. Overall water quality appears to be high; there is no evidence of sustained bloom conditions or other causes of prolonged turbidity. 

```{r mld_number_secchi_depths_by_year_barplot, fig.height = 6, fig.width = 7, dev.args = list(pointsize = 12)}
# barplot: number of samples per year 
ggplot(mld, aes(y = year)) + geom_bar() + ggtitle("Number of Secchi Depth Samples by Year")
```

Note that we have missing data for the years 1977, 1983, 1992, and 2004. These years could be represented by 'NA' when we process the data for analysis; but this is not necessary given the rank-order basis of the Mann-Kendall analysis.  

```{r}
# compute secchi depth mean by year
tapply(mld$secchi_m, mld$year, mean)
```

```{r}
# plot mean secchi depth by year, excluding years with no data from x-axis

# create vector for years excluding 1977, 1983, 1992, 2004
year_seq <-seq(from = 1976, to = 2022, by = 1)
remove <-c(1977, 1983, 1992, 2004)
year_seq %in% remove
year_seq <-year_seq[! year_seq %in% remove]

# visualize mean secchi depth by year in scatterplot
# use rounded values for mean (0.1 m resolution)
plot(year_seq, round(tapply(mld$secchi_m, mld$year, mean), 1), pch = 19, col = "blue", main = "Mean Secchi Depth by Year", xlab = "Year", ylab = "Mean Secchi Depth (m)")


```



This is probably not the best approach to visualization. It artificially condenses the x-axis by removing the missing years (1977, 1983, 1992, and 2004). 

A better approach would be to keep all years in the x-axis time series, but add NA values in the positions corresponding to mean Secchi depth for the missing years.

The computation of the annual mean may also be sensitive to the method of data aggregration. The visualization above computes the annual mean based on all observations for a given year. Jeremy Weeds proposes computing the annual mean by finding the mean for each month in a given year, and then computing the mean of all monthly means.

For now, I'm going to stick with a simple arithmetic mean based on all observations within a given year.

```{r}
# mean Secchi depth by year with dplyr

# organize means into dateframe by year
secchi_means_df <-mld %>%
  group_by(year) %>%
  summarize(annual_mean = mean(secchi_m))

# scatterplot: annual means by year excluding 1977, 1983, 1992, 2004
ggplot(secchi_means_df, aes(x = year, y = annual_mean)) + geom_point(color = "blue") + ggtitle("Mean Secchi Depth by Year") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


There does appear to be a modest improvement in water transparency (i.e., greater clarity, larger Secchi depths) over time. The inflection point appears to be around the year 2000, primarily because of the unusually low means (Secchi depth < 4 m) in 1996 and 1998. 

Without these two years, the data look much more like random scatter with a zero slope. Sensitivity analysis may be useful given the potential outliers (1996, 1998) and the missing data. Also, I did not bother to round the annual means for this plot.

################################################
#### TREND ANALYSIS                         ####
#### SECCHI DEPTHS                          ####
################################################

Let's start by determing whether there are any ties (i.e., identical means for more than one year) in the Secchi depth data set.

```{r}
# look at means
secchi_means_df$annual_mean
```
But visually identifying identical values is slow. Let's check.

```{r}
duplicated(secchi_means_df$annual_mean)
```
We have one tie; it involves two different years. Let's take a look at the specific years with equal means.

```{r}
secchi_means_df[duplicated(secchi_means_df$annual_mean), ]

```
1986 and 1994 both have mean values of 4.7 meters. 

Note that the number of ties is sensitive to rounding. I used mean values to six digits to check for ties; but if we round the annual means, we will probably have more identical values.

```{r}
# add a column with rounded annual means
secchi_means_df$rounded_annual_mean <-round(secchi_means_df$annual_mean, digits = 1)
secchi_means_df

```

```{r}
# check for identical values
duplicated(secchi_means_df$rounded_annual_mean)
```

We have a lot more ties, when we round the mean annual values to reflect the likely precision of the field values. Let's take a look at which rows are involved in ties.

```{r}
secchi_means_df[duplicated(secchi_means_df$rounded_annual_mean), ]

```
With rounding, we have 25 observations of the 43 involved in ties! The data look very different depending on whether we round the observations at each stage (round raw observations, round annual means after computation).

So I'm curious to see how sensitive the results are to the rounding process.

#### (1A) Run a Mann-Kendall trend analysis on yearly averages (May – October) on the full dataset WITHOUT rounding the annual means.

```{r}
# compute Kendall's tau to determine degree of association 
# between annual mean Secchi depth (m) and year (character data) 
# for most years: 1976-2022, except for 1977, 1983, 1992, and 2004
with(secchi_means_df, Kendall(year, annual_mean))
```
The hypothesis test provides fairly strong evidence that there is a positive trend. 

```{r}
# complete results for Mann-Kendall trend test
summary(with(secchi_means_df, Kendall(year, annual_mean)))
```
S is positive, indicating that the overall trend is positive. 

Per R documentation for "Kendall," S is the Kendall Score.

In Helsel et al. 2020, Ch. 8, p. 218, n * (n - 1)/2 specifies the total number of comparisons that can be made between pairs of rows in the dataframe. For each pair of rows, we look to see whether the differences between X (years) and Y (mean annual Secchi depth) are concordant--i.e., both positive, or both negative. 

Per Helsel et al., 2020, p. 217, the number of concordant pairs is denoted P.

By contrast, if we compare two rows and find the differences for X and Y have opposite signs (one positive, one negative), the differences are discordant. The number of discordant pairs is denoted M.

S is defined as the value of P - M.

For our sample data, we have n = 43. 

Let's look at two hypothetical trends for this sample size, one monotonic increasing and one monotonic decreasing. 

```{r}
# value of S with all observations concordant for n = 43
n <-43
S_all_concordant <-n * (n - 1)/2
S_all_concordant
```
Per equation 8.6 (p. 218) in Helsel et al., 2020, the formula for Tau is

Tau = S / (n * (n - 1)/2)

```{r}
# value of Tau with all observations concordant for n = 43
tau = S_all_concordant/(n * (n - 1)/2)
tau
```
Note that the sign of the first n term changes in the computation of S for a monotonically decreasing trend: the value is -n.

```{r}
# value of S with all observations discordant for n = 43
S_all_discordant <- -n * (n - 1)/2
S_all_discordant
```

So we end up with a value of Tau that is negative.

```{r}
# value of Tau with all observations concordant for n = 43
tau = S_all_discordant/(n * (n - 1)/2)
tau
```
These formulae apply for a case in which there are no ties.

I'm wondering whether the "denominator" reported in the summary is equal to n * (n - 1)/2 for a case with no ties.  

We can try a toy dataset that is monotonically increasing to check this out.

```{r}
# toy dataframe: monotonic increase, no ties
toy_year <-c(2005:2020)
toy_year <-as.factor(toy_year)
toy_year <-factor(toy_year, ordered = TRUE, levels = c("2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020"))
toy_y <-c(seq(100:115))
mono_plus <-data.frame(toy_year, toy_y)
mono_plus
```

```{r}
# define n (number of rows) for monotonically increasing toy data
n <-dim(mono_plus)[1]
n
```
```{r}
# compute Kendall's tau to determine degree of association 
# for toy data with monotonically increasing trend 
with(mono_plus, Kendall(toy_year, toy_y))
```
```{r}
# summary of results for Mann-Kendall analysis  
# for toy data with monotonically increasing trend 
summary(with(mono_plus, Kendall(toy_year, toy_y)))
```

```{r}
# is reported "denominator" equal to n * (n - 1)/2 given positive trend?
n * (n - 1)/2
```
Yes, that's the denominator from the summary. 

S is equal to the denominator, hence we have a value of tau = 1 (as one would expect).

But with our Secchi depth data, the denominator is not computed in such a straightforward manner. We have two identical values (one pair of observations with a tie). The value of the denominator with n = 43 for the lakes data is 901.4988--not even a whole number. 

```{r}
# Denominator with n = 43 for no ties:
n = 43
n * (n - 1)/2
```
Presumably the denominator was computed with the approximation/continuity correction mentioned in the documentation (per Kendall 1976, equation 4.4, p. 55). The impact of the ties makes the denominator just a little smaller: 901.4988 is about 0.16% smaller than full denominator, 903. This means the value of Tau will be just a tiny bit larger.

```{r}
# tau for lakes data with correction for ties
summary(with(secchi_means_df, Kendall(year, annual_mean)))

# tau for lakes data without correction for ties
uncorrected_tau <-246/(n * (n - 1)/2)
uncorrected_tau
```
Failing to correct for ties results in a slightly lower value of tau in [1]: 0.2724252. The uncorrected summary statistic indicates that the trend is not quite as strong as it would be, were the correction for ties applied correctly. But the impacts are extremely modest. It is hard to imagine a situation where this would have a substantial impact--one would need fairly large proportion of identical values for mean annual Secchi depth before the correction would have any impact. 

The rounded annual means provide an example of such a case.

#### (1B) Run a Mann-Kendall trend analysis on yearly averages (May – October) on the full dataset AFTER rounding the annual means.

```{r}
# compute Kendall's tau to determine degree of association 
# between rounded annual mean Secchi depth (m) and year (character data) 
# for most years: 1976-2022, except for 1977, 1983, 1992, and 2004
with(secchi_means_df, Kendall(year, rounded_annual_mean))

```
These results are counterintuitive to me. With so many identical annual means (ties), it seems like there should be less certainty about any potential trend. But the summary statistic for the data with many ties (25 of 43 rows match at least one other row) suggests there is a slightly stronger trend (tau = 0.3 with large proportion of ties, tau = 0.273 with small proportion of ties).

Clarify impact of ties? I'm not sure why we see these differences.

```{r}
summary(with(secchi_means_df, Kendall(year, rounded_annual_mean)))
```
The denominator is much lower with lots of ties (about 880) than with just a pair of identical observations (about 901). The correction for ties definitely drives down the denominator used to compute Tau. But the score is much higher (264 with many ties, 246 for one tie).

The variance of the score is somewhat lower with many ties (9064 vs. about 9127 with just one tie).

The results are surprisingly similar, even with 25 different identical observations generating many ties in the second analysis. The conclusion that there is a modest positive trend seems reasonable regardless of whether we round the mean annual Secchi depths. 

#### (1C) Run a Mann-Kendall trend analysis on yearly averages (May – October) on the full dataset with year as an ORDERED FACTOR.

```{r}
str(secchi_means_df)
```
Even though we started off with date data, the values for "year" ended up being character data. I think of Kendall's Tau as a tool for dealing with ordered factors, so I'm going to convert the years to values of an ordered factor, and see whether this changes our results.

```{r}
# convert year from character data to ordered factor
secchi_means_df$year_ORD <-as.factor(secchi_means_df$year)
secchi_means_df$year_ORD <-factor(secchi_means_df$year_ORD, ordered = TRUE, levels = c("1976", "1978", "1979", "1980", "1981", "1982", "1984", "1985", "1986", "1987", "1988", "1989", "1990", "1991", "1993", "1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022"))
  
str(secchi_means_df)

```

```{r}
# compute Kendall's tau to determine degree of association 
# between rounded annual mean Secchi depth (m) and year (ordered factor) 
# for most years: 1976-2022, except for 1977, 1983, 1992, and 2004
with(secchi_means_df, Kendall(year_ORD, rounded_annual_mean))

```
```{r}
# complete results
summary(with(secchi_means_df, Kendall(year_ORD, rounded_annual_mean)))

```
These results are identical to those computed with "year" coded as character data.

We can double-check this by using the unrounded annual means with the ordered factor:

```{r}
# compute Kendall's tau to determine degree of association 
# between annual mean Secchi depth (m) and year (ordered factor) 
# for most years: 1976-2022, except for 1977, 1983, 1992, and 2004
with(secchi_means_df, Kendall(year_ORD, annual_mean))
```
```{r}
# results
summary(with(secchi_means_df, Kendall(year_ORD, annual_mean)))
```
#### (1D) Run trend analysis on yearly averages (May – October) on the full dataset with year as a NUMERIC variable

The R documentation indicates x and y should be numeric vectors or factors. Does it make a difference, if "year" is encoded as a numeric variable rather than as character data or as an ordered factor?

```{r}
# compute Kendall's tau to determine degree of association 
# between annual mean Secchi depth (m) and year (ordered factor) 
# for most years: 1976-2022, except for 1977, 1983, 1992, and 2004
with(secchi_means_df, Kendall(as.numeric(year), annual_mean))

```
The way in which the test statistic is computed does not appear to be sensitive to the status of "year." The results for "year" encoded as an ordered factor, as character data, or as numeric values are identical. 

We can, in theory, apply cor.test to characterize the degree of association.

```{r}
# compare: results with cor.test and numeric data for year
with(secchi_means_df, cor.test(as.numeric(year), annual_mean), method = "kendall")

```
The correlation coefficient suggests a low-moderate strength of association. 

```{r}
# compare: results with cor.test and numeric data for year
with(secchi_means_df, cor.test(as.numeric(year), annual_mean), method = "kendall", continuity = TRUE)

```
Applying the continuity correction recommend in Helsel et al. (2020), Ch. 8, does not have any impact on the reported results.

The scale of the correlation coefficient does appear to be different (0.40 with cor.test(), 0.27-0.30 with Kendall(). I'm not sure how to think about these differences.





(2) Run a Mann-Kendall trend analysis on yearly averages (May – October) on the most recent 10 years in the dataset; and

(3) Run a Mann-Kendall trend analysis on late summer averages (we’ll use August 1 – September 7) on the full dataset.


```{r}
# subset data between August 1 and September 7 for each year

# view peak chlorophyll data
mld %>% filter(date >= as.Date(paste(year, 08, 01, sep = "-")), date <=as.Date(paste(year, 09, 07, sep = "-")))

# create df with peak chlorophyll data
peak_mld <-mld %>% filter(date >= as.Date(paste(year, 08, 01, sep = "-")), date <=as.Date(paste(year, 09, 07, sep = "-")))

# code source:
# https://stackoverflow.com/questions/73534376/filtering-data-by-month-and-day-with-pasted-year

```
We are missing observations for 1976 and 1977 in the target time period. Probably a good idea to confirm the filter is working properly.

```{r}
# check: do we really have no data between August 1 and September 7 for 1976?
peak_mld_1976 <-mld %>% 
  group_by(year) %>%
  filter(date >= as.Date("1976-08-01") & date <= as.Date("1976-09-07"))
peak_mld_1976
```

Absence of data in target date range confirmed.

```{r}
# check: do we really have no data between August 1 and September 7 for 1976?
peak_mld_1977 <-mld %>% 
  group_by(year) %>%
  filter(date >= as.Date("1977-08-01") & date <= as.Date("1977-09-07"))
peak_mld_1977
```

Absence of data in target date range confirmed.

Let's see which years are missing data.

```{r peak_mld_number_secchi_depths_by_year_barplot, fig.height = 6, fig.width = 7, dev.args = list(pointsize = 12)}
# barplot: number of samples per year 
ggplot(peak_mld, aes(y = year)) + geom_bar() + ggtitle("Number of Secchi Depth Samples by Year")
```

We have missing data for 2020, 2004, 1992, 1991, 1983, 1977, and 1976. Quite a bit more uncertainty given the absence of data for 7 of the 47-year time range (1976-2022).

Furthermore, the means are computed based on a single observation for several years: 2021, 1994, 1993, 1986, 1984, 1981, and 1980. For most years, there is very little data within this time period. Only 13 years (1985, 1987, 1988, and 2010-2019) have a means based on more than three observations. For over two-thirds of the observation, the estimate of central tendency is based on sparse data between August 1 and September 7.

```{r}
# means by year for peak chlorophyll (August 1 - September 7)
tapply(peak_mld$secchi_m, peak_mld$year, mean)

dim(tapply(peak_mld$secchi_m, peak_mld$year, mean))
```

This table shows the mean of the peak chlorophyll period for 40 of the 47 years.

Let's visualize the mean Sechhi depth by year.


```{r}
# mean Secchi depth by year with dplyr

# organize means into dateframe by year
secchi_means_df_peak <-peak_mld %>%
  group_by(year) %>%
  summarize(annual_mean_peak = mean(secchi_m))

# scatterplot: annual means by year excluding 1977, 1983, 1992, 2004
ggplot(secchi_means_df_peak, aes(x = year, y = annual_mean_peak)) + geom_point(color = "blue") + ggtitle("Mean Secchi Depth by Year \n August 1 - September 7") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
Let's round the annual means. (To be consistent with the earlier analyses, I should probably round the raw Secchi depths, too--but I won't bother for now. The rounding doesn't seem to make much difference.)

```{r}
# add a column with rounded annual means
secchi_means_df_peak$rounded_annual_mean <-round(secchi_means_df_peak$annual_mean_peak, digits = 1)
secchi_means_df_peak

```
After rounding, we do not have any ties (identical values in at least two rows) in the peak chlorophyll data frame.

```{r}
trim_peak <-data.frame(secchi_means_df_peak$year, secchi_means_df_peak$rounded_annual_mean )

duplicated(trim_peak)
```
Let' see whether we still have a notable trend given the missing data.

```{r}
# compute Kendall's tau to determine degree of association 
# between rounded annual mean Secchi depth (m) and year (ordered factor) 
# for peak chlorophyll: August 1 - September 7
# 7 years NA, most years have sparse data (n < 4 for mean computation)
with(secchi_means_df_peak, Kendall(year, rounded_annual_mean))

```
We still have a weakly positive trend and a signficant p-value.

```{r}
# complete results
summary(with(secchi_means_df_peak, Kendall(year, rounded_annual_mean)))

```
The value of S is notably lower, as well as the value of the denominator (n * (n - 1)/2), with n = 40.


# Ben's code for months
Dmeanmonth <- SDdata |> 
  group_by(Year, Month) |> 
  summarize(monmeanSD = mean(SDT_m, na.rm = T))
SDannmean <- SDmeanmonth |> 
  group_by(Year) |> 
  summarize(annmeanSD = mean(monmeanSD, na.rm = T))
has context menu


